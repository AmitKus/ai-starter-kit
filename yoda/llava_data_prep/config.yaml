llm: 
    "api": "sambastudio"
    "do_sample": True
    "temperature": 0.7
    "streaming": False
    "max_tokens_to_generate": 1200
    "coe": True #set as true if using Sambastudio CoE endpoint
    "select_expert": "Meta-Llama-3.1-70B-Instruct" #set if using sncloud, SambaStudio CoE llm expert
    "process_prompt": False

prompts:
    "table_modification": "yoda/prompts/llama3_1-modify-table.yaml"
    "ocr_query_answer": "yoda/prompts/llama3_1-table-ocr.yaml"